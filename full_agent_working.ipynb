{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE THIS FILE\n",
    "# This was a intial work, where agent does everything\n",
    "# This is the worst case scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Imports and Basic Setup\n",
    "# Import required libraries for data processing, database operations, language models, and environment variables\n",
    "import os\n",
    "from typing import Dict, List, Optional, TypedDict, Literal, Union, Annotated\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import json\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AnyMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT\n",
    "import sqlite3\n",
    "import re\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Load API keys from environment file\n",
    "load_dotenv('api_key.env')\n",
    "\n",
    "# Initialize memory for state management\n",
    "memory = {}  # Using a simple dictionary for in-memory storage\n",
    "\n",
    "# Part 2: Type Definitions and Base Classes\n",
    "class QueryType(Enum):\n",
    "    DIRECT_SQL = \"direct_sql\" \n",
    "    ANALYSIS = \"analysis\"\n",
    "\n",
    "@dataclass\n",
    "class QueryClassification:\n",
    "    type: QueryType\n",
    "    explanation: str\n",
    "    raw_response: str\n",
    "    confidence: float = 1.0\n",
    "\n",
    "class AnalysisState(TypedDict):\n",
    "    user_query: str\n",
    "    query_classification: Dict\n",
    "    decomposed_questions: List[str]\n",
    "    sql_results: Dict\n",
    "    analysis: str\n",
    "    final_output: Dict\n",
    "    token_usage: Dict\n",
    "    processing_time: float\n",
    "    agent_states: Dict\n",
    "    raw_responses: Dict\n",
    "    messages: List[AnyMessage]\n",
    "\n",
    "class ConfigError(Exception):\n",
    "    \"\"\"Custom exception for configuration errors\"\"\"\n",
    "    pass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    db_path: str = \"final_working_database.db\"\n",
    "    sqlite_path: str = \"sqlite:///final_working_database.db\"\n",
    "    model_name: str = \"claude-3-sonnet-20240229\"\n",
    "    confidence_threshold: float = 0.85  # High confidence threshold for autonomous decisions\n",
    "    \n",
    "    @property\n",
    "    def api_key(self) -> str:\n",
    "        api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ConfigError(\"ANTHROPIC_API_KEY not found in api_key.env file\")\n",
    "        return api_key\n",
    "\n",
    "# Part 3: Prompt Templates\n",
    "QUERY_CLASSIFIER_PROMPT = \"\"\"You are a query classifier that determines if a stock market question:\n",
    "1. Can be answered with a direct SQL query\n",
    "2. Needs complex analysis\n",
    "\n",
    "Respond in JSON format:\n",
    "{\n",
    "    \"type\": \"direct_sql\" | \"analysis\",\n",
    "    \"explanation\": \"brief explanation of classification\",\n",
    "    \"confidence\": <float between 0-1>,\n",
    "    \"needs_clarification\": {\n",
    "        \"required\": <boolean>,\n",
    "        \"details\": \"description of ambiguity or missing information\",\n",
    "        \"suggested_questions\": [\"list of clarifying questions\"]\n",
    "    }\n",
    "}\"\"\"\n",
    "\n",
    "SQL_AGENT_PROMPT = \"\"\"You are an expert financial database analyst. Your task is to:\n",
    "1. Analyze stock market queries\n",
    "2. Create appropriate SQL queries using the provided database schema\n",
    "3. Provide clear results\n",
    "\n",
    "If you encounter any ambiguity or data limitations:\n",
    "1. Clearly explain the issue\n",
    "2. Specify what clarification is needed\n",
    "3. Suggest possible alternatives\n",
    "\n",
    "Your responses should include:\n",
    "1. Confidence level (0-1)\n",
    "2. Any clarification needed\n",
    "3. Thought process\n",
    "4. SQL query (if possible)\n",
    "5. Result interpretation\"\"\"\n",
    "\n",
    "ANALYST_PROMPT = \"\"\"You are an expert financial analyst. Analyze the provided SQL results and provide insights.\n",
    "\n",
    "If you encounter:\n",
    "- Unclear patterns\n",
    "- Multiple possible interpretations\n",
    "- Need for additional context\n",
    "- Insufficient data\n",
    "\n",
    "Clearly state:\n",
    "1. What additional information would help\n",
    "2. Why it's needed\n",
    "3. How it would improve the analysis\n",
    "\n",
    "Focus on:\n",
    "1. Price trends and patterns\n",
    "2. Volume analysis\n",
    "3. Technical indicators\n",
    "4. Risk assessment\n",
    "5. Notable patterns\n",
    "\n",
    "Be specific and data-driven in your analysis.\"\"\"\n",
    "\n",
    "# Part 4: Main StockAnalyzer Class\n",
    "class StockAnalyzer:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.conn = sqlite3.connect(config.db_path)\n",
    "        self.schema = self._get_database_schema()\n",
    "        self.db = self._init_database()\n",
    "        self.llm = self._init_llm()\n",
    "        \n",
    "        self.classifier_memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        self.sql_memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        self.analyst_memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True\n",
    "        )\n",
    "        \n",
    "        self.sql_agent = self._setup_sql_agent()\n",
    "        self.token_usage = {\"prompt_tokens\": 0, \"completion_tokens\": 0}\n",
    "        self.anthropic_client = Anthropic(api_key=config.api_key)\n",
    "        self.agent_states = {}\n",
    "        self.raw_responses = {}\n",
    "        self.query_cache = {}\n",
    "\n",
    "    def _init_database(self) -> SQLDatabase:\n",
    "        try:\n",
    "            return SQLDatabase.from_uri(self.config.sqlite_path)\n",
    "        except Exception as e:\n",
    "            raise ConfigError(f\"Database initialization failed: {str(e)}\")\n",
    "\n",
    "    def _get_database_schema(self) -> str:\n",
    "        try:\n",
    "            cursor = self.conn.cursor()\n",
    "            tables = cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n",
    "            \n",
    "            schema = []\n",
    "            for table in tables:\n",
    "                table_name = table[0]\n",
    "                columns = cursor.execute(f\"PRAGMA table_info({table_name});\").fetchall()\n",
    "                schema.append(f\"Table: {table_name}\")\n",
    "                schema.append(\"Columns:\")\n",
    "                for col in columns:\n",
    "                    schema.append(f\"  - {col[1]} ({col[2]})\")\n",
    "                schema.append(\"\")\n",
    "                \n",
    "            return \"\\n\".join(schema)\n",
    "        except Exception as e:\n",
    "            raise ConfigError(f\"Failed to get database schema: {str(e)}\")\n",
    "\n",
    "    def _init_llm(self) -> ChatAnthropic:\n",
    "        return ChatAnthropic(\n",
    "            model=self.config.model_name,\n",
    "            temperature=0,\n",
    "            api_key=self.config.api_key\n",
    "        )\n",
    "\n",
    "    def _setup_sql_agent(self):\n",
    "        toolkit = SQLDatabaseToolkit(db=self.db, llm=self.llm)\n",
    "        return create_sql_agent(\n",
    "            llm=self.llm,\n",
    "            toolkit=toolkit,\n",
    "            agent_type=\"zero-shot-react-description\",\n",
    "            verbose=True,\n",
    "            memory=self.sql_memory,\n",
    "            prefix=SQL_AGENT_PROMPT\n",
    "        )\n",
    "\n",
    "    # def _get_user_clarification(self, prompt: str) -> str:\n",
    "    #     return input(f\"\\n{prompt}\\nPlease provide clarification: \")\n",
    "\n",
    "    def analyze(self, query: str) -> Dict:\n",
    "        if query in self.query_cache:\n",
    "            print(\"Using cached results...\")\n",
    "            return self.query_cache[query]\n",
    "            \n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            self.token_usage = {\"prompt_tokens\": 0, \"completion_tokens\": 0}\n",
    "            self.agent_states = {}\n",
    "            self.raw_responses = {}\n",
    "\n",
    "            classification = self._classify_query(query)\n",
    "            \n",
    "            if classification.type == QueryType.DIRECT_SQL and classification.confidence >= self.config.confidence_threshold:\n",
    "                result = self._direct_sql_query(query)\n",
    "                self.query_cache[query] = result\n",
    "                return result\n",
    "            \n",
    "            decomposed_questions = self._decompose_question(query)\n",
    "            sql_results = self._run_sql_analysis(decomposed_questions)\n",
    "            \n",
    "            # # Only ask for clarification if there are errors\n",
    "            # for result in sql_results.values():\n",
    "            #     if isinstance(result.get('result'), str) and 'error' in result.get('result', '').lower():\n",
    "            #         clarification = self._get_user_clarification(\n",
    "            #             f\"Error in SQL execution: {result['result']}\\nHow would you like to proceed?\"\n",
    "            #         )\n",
    "            #         if clarification:\n",
    "            #             result = self._retry_sql_query(result['question'], clarification)\n",
    "            #             if result:\n",
    "            #                 sql_results[result['question']] = result\n",
    "            \n",
    "            analysis = self._analyze_results(query, sql_results)\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            final_output = {\n",
    "                \"query_type\": classification.type.value,\n",
    "                \"user_query\": query,\n",
    "                \"query_classification\": {\n",
    "                    \"type\": classification.type.value,\n",
    "                    \"explanation\": classification.explanation,\n",
    "                    \"confidence\": classification.confidence,\n",
    "                    \"raw_response\": classification.raw_response\n",
    "                },\n",
    "                \"sub_questions\": decomposed_questions,\n",
    "                \"sql_analysis\": sql_results,\n",
    "                \"expert_analysis\": analysis,\n",
    "                \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "                \"token_usage\": self.token_usage,\n",
    "                \"processing_time\": processing_time,\n",
    "                \"agent_states\": self.agent_states,\n",
    "                \"raw_responses\": self.raw_responses\n",
    "            }\n",
    "            \n",
    "            filename = f\"{query[:50].replace(' ', '_').lower()}_analysis.json\"\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(final_output, f, indent=2)\n",
    "                \n",
    "            self.query_cache[query] = final_output\n",
    "            return final_output\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e), \"query\": query}\n",
    "        finally:\n",
    "            self.conn.close()\n",
    "\n",
    "    def _classify_query(self, query: str) -> QueryClassification:\n",
    "        chat_history = self.classifier_memory.load_memory_variables({})[\"chat_history\"]\n",
    "        for message in chat_history:\n",
    "            if isinstance(message, HumanMessage) and query.lower() in message.content.lower():\n",
    "                print(\"Using cached classification...\")\n",
    "                return self.query_cache.get(message.content, {}).get(\"query_classification\")\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke([\n",
    "                SystemMessage(content=QUERY_CLASSIFIER_PROMPT),\n",
    "                HumanMessage(content=f\"Classify this question: {query}\")\n",
    "            ])\n",
    "            \n",
    "            self.classifier_memory.save_context(\n",
    "                {\"input\": query},\n",
    "                {\"output\": response.content}\n",
    "            )\n",
    "            \n",
    "            self._update_token_usage(response)\n",
    "            classification = json.loads(response.content)\n",
    "            \n",
    "            self.raw_responses['classification'] = response.content\n",
    "            \n",
    "            # # Only ask for clarification if confidence is low\n",
    "            # if classification.get('confidence', 1.0) < 0.5:\n",
    "            #     details = classification.get('needs_clarification', {}).get('details', '')\n",
    "            #     questions = classification.get('needs_clarification', {}).get('suggested_questions', [])\n",
    "            #     clarification = self._get_user_clarification(\n",
    "            #         f\"Low confidence in classification. {details}\\n\\nSuggested questions:\\n\" + \n",
    "            #         \"\\n\".join(f\"- {q}\" for q in questions)\n",
    "            #     )\n",
    "            #     return self._classify_query(f\"{query} {clarification}\")\n",
    "            \n",
    "            return QueryClassification(\n",
    "                type=QueryType(classification['type']),\n",
    "                explanation=classification['explanation'],\n",
    "                confidence=classification.get('confidence', 1.0),\n",
    "                raw_response=response.content\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return QueryClassification(\n",
    "                type=QueryType.ANALYSIS,\n",
    "                explanation=f\"Classification failed: {str(e)}\",\n",
    "                confidence=0.0,\n",
    "                raw_response=str(e)\n",
    "            )\n",
    "\n",
    "    def _direct_sql_query(self, query: str) -> Dict:\n",
    "        chat_history = self.sql_memory.load_memory_variables({})[\"chat_history\"]\n",
    "        for message in chat_history:\n",
    "            if isinstance(message, HumanMessage) and query.lower() in message.content.lower():\n",
    "                print(\"Using cached SQL query results...\")\n",
    "                return self.query_cache.get(message.content, {})\n",
    "        \n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            result = self.sql_agent.invoke({\"input\": query})\n",
    "            self._update_token_usage(result)\n",
    "            \n",
    "            self.agent_states['direct_sql'] = result\n",
    "            \n",
    "            thought = self._extract_thought(result['output'])\n",
    "            sql = self._extract_sql(result['output'])\n",
    "            \n",
    "            # # Only ask for clarification if SQL generation fails\n",
    "            # if not sql:\n",
    "            #     clarification = self._get_user_clarification(\n",
    "            #         \"Could not generate SQL query. Please provide guidance on what data you're looking for:\"\n",
    "            #     )\n",
    "            #     result = self.sql_agent.invoke({\"input\": f\"{query} {clarification}\"})\n",
    "            #     sql = self._extract_sql(result['output'])\n",
    "            \n",
    "            try:\n",
    "                sql = sql.split(';')[0] + ';'\n",
    "                df = pd.read_sql_query(sql, self.conn)\n",
    "                formatted_results = df.to_dict('records')\n",
    "            except Exception as e:\n",
    "                # # Only ask for clarification if SQL execution fails\n",
    "                # clarification = self._get_user_clarification(\n",
    "                #     f\"Error executing SQL: {str(e)}\\nHow would you like to modify the query?\"\n",
    "                # )\n",
    "                # try:\n",
    "                #     df = pd.read_sql_query(clarification, self.conn)\n",
    "                #     formatted_results = df.to_dict('records')\n",
    "                # except Exception as e2:\n",
    "                formatted_results = f\"Error executing SQL: {str(e)}\"\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            final_result = {\n",
    "                \"query_type\": \"direct_sql\",\n",
    "                \"user_query\": query,\n",
    "                \"thought_process\": thought,\n",
    "                \"sql_query\": sql,\n",
    "                \"results\": formatted_results,\n",
    "                \"raw_agent_output\": result['output'],\n",
    "                \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "                \"token_usage\": self.token_usage,\n",
    "                \"processing_time\": processing_time,\n",
    "                \"agent_state\": result\n",
    "            }\n",
    "            \n",
    "            self.sql_memory.save_context(\n",
    "                {\"input\": query},\n",
    "                {\"output\": json.dumps(final_result)}\n",
    "            )\n",
    "            \n",
    "            return final_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e), \"query\": query}\n",
    "\n",
    "    def _decompose_question(self, query: str) -> List[str]:\n",
    "        response = self.llm.invoke([\n",
    "            SystemMessage(content=\"Break down this stock analysis question into specific sub-questions that can be answered with SQL queries:\"),\n",
    "            HumanMessage(content=query)\n",
    "        ])\n",
    "        \n",
    "        self._update_token_usage(response)\n",
    "        self.raw_responses['decomposition'] = response.content\n",
    "        \n",
    "        questions = [\n",
    "            q.strip().split(\". \", 1)[1] if \". \" in q else q.strip()\n",
    "            for q in response.content.split(\"\\n\")\n",
    "            if q.strip() and q[0].isdigit()\n",
    "        ]\n",
    "        \n",
    "        # # Only ask for clarification if no questions were generated\n",
    "        # if not questions:\n",
    "        #     clarification = self._get_user_clarification(\n",
    "        #         \"Could not break down the question. Please specify what aspects you want to analyze:\"\n",
    "        #     )\n",
    "        #     return self._decompose_question(f\"{query} {clarification}\")\n",
    "        \n",
    "        return questions\n",
    "\n",
    "    def _run_sql_analysis(self, questions: List[str]) -> Dict:\n",
    "        results = {}\n",
    "        agent_states = {}\n",
    "        \n",
    "        for i, question in enumerate(questions, 1):\n",
    "            chat_history = self.sql_memory.load_memory_variables({})[\"chat_history\"]\n",
    "            cached_result = None\n",
    "            for message in chat_history:\n",
    "                if isinstance(message, HumanMessage) and question.lower() in message.content.lower():\n",
    "                    print(f\"Using cached results for sub-question {i}...\")\n",
    "                    cached_result = self.query_cache.get(message.content)\n",
    "                    break\n",
    "            \n",
    "            if cached_result:\n",
    "                results[f\"question_{i}\"] = cached_result\n",
    "                continue\n",
    "            try:\n",
    "                result = self.sql_agent.invoke({\"input\": question})\n",
    "                self._update_token_usage(result)\n",
    "                \n",
    "                agent_states[f\"question_{i}\"] = result\n",
    "                \n",
    "                thought = self._extract_thought(result['output'])\n",
    "                sql = self._extract_sql(result['output'])\n",
    "                \n",
    "                if not sql:\n",
    "                    clarification = self._get_user_clarification(\n",
    "                        f\"Could not generate SQL for: {question}\\nPlease provide guidance:\"\n",
    "                    )\n",
    "                    result = self.sql_agent.invoke({\"input\": f\"{question} {clarification}\"})\n",
    "                    sql = self._extract_sql(result['output'])\n",
    "                \n",
    "                try:\n",
    "                    sql = sql.split(';')[0] + ';'\n",
    "                    df = pd.read_sql_query(sql, self.conn)\n",
    "                    parsed_result = df.to_dict('records')\n",
    "                except Exception as e:\n",
    "                    clarification = self._get_user_clarification(\n",
    "                        f\"Error executing SQL for: {question}\\n{str(e)}\\nHow would you like to modify the query?\"\n",
    "                    )\n",
    "                    try:\n",
    "                        df = pd.read_sql_query(clarification, self.conn)\n",
    "                        parsed_result = df.to_dict('records')\n",
    "                        sql = clarification\n",
    "                    except Exception as e2:\n",
    "                        parsed_result = f\"Error executing SQL even after clarification: {str(e2)}\"\n",
    "                \n",
    "                results[f\"question_{i}\"] = {\n",
    "                    \"question\": question,\n",
    "                    \"thought\": thought if thought else \"No thought process provided\",\n",
    "                    \"sql\": sql if sql else \"No SQL query provided\",\n",
    "                    \"result\": parsed_result,\n",
    "                    \"raw_output\": result['output']\n",
    "                }\n",
    "                    \n",
    "            except Exception as e:\n",
    "                results[f\"question_{i}\"] = {\n",
    "                    \"error\": str(e),\n",
    "                    \"question\": question\n",
    "                }\n",
    "        \n",
    "        self.agent_states['sql_analysis'] = agent_states\n",
    "        return results\n",
    "\n",
    "    def _analyze_results(self, query: str, sql_results: Dict) -> str:\n",
    "        results_context = json.dumps(sql_results, indent=2)\n",
    "        response = self.llm.invoke([\n",
    "            SystemMessage(content=ANALYST_PROMPT),\n",
    "            HumanMessage(content=f\"\"\"\n",
    "            Original Question: {query}\n",
    "            \n",
    "            Analysis Results:\n",
    "            {results_context}\n",
    "            \n",
    "            Provide a comprehensive analysis.\"\"\")\n",
    "        ])\n",
    "        \n",
    "        self._update_token_usage(response)\n",
    "        self.raw_responses['analysis'] = response.content\n",
    "        \n",
    "        if len(response.content.strip()) < 100:  # If analysis is too short\n",
    "            clarification = self._get_user_clarification(\n",
    "                \"Analysis seems incomplete. What specific aspects would you like to focus on?\"\n",
    "            )\n",
    "            return self._analyze_results(f\"{query} {clarification}\", sql_results)\n",
    "        \n",
    "        return response.content\n",
    "\n",
    "    def _retry_sql_query(self, question: str, clarification: str) -> Dict:\n",
    "        try:\n",
    "            result = self.sql_agent.invoke({\"input\": f\"{question} {clarification}\"})\n",
    "            sql = self._extract_sql(result['output'])\n",
    "            \n",
    "            if sql:\n",
    "                df = pd.read_sql_query(sql, self.conn)\n",
    "                return {\n",
    "                    \"question\": question,\n",
    "                    \"thought\": self._extract_thought(result['output']),\n",
    "                    \"sql\": sql,\n",
    "                    \"result\": df.to_dict('records'),\n",
    "                    \"raw_output\": result['output']\n",
    "                }\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    def _update_token_usage(self, response):\n",
    "        if hasattr(response, '_raw_response') and 'usage' in response._raw_response:\n",
    "            usage = response._raw_response['usage']\n",
    "            self.token_usage[\"prompt_tokens\"] += usage.get('input_tokens', 0)\n",
    "            self.token_usage[\"completion_tokens\"] += usage.get('output_tokens', 0)\n",
    "        elif isinstance(response, dict) and 'usage' in response:\n",
    "            usage = response['usage']\n",
    "            self.token_usage[\"prompt_tokens\"] += usage.get('input_tokens', 0)\n",
    "            self.token_usage[\"completion_tokens\"] += usage.get('output_tokens', 0)\n",
    "        elif hasattr(response, 'usage'):\n",
    "            usage = response.usage\n",
    "            self.token_usage[\"prompt_tokens\"] += usage.input_tokens if hasattr(usage, 'input_tokens') else 0\n",
    "            self.token_usage[\"completion_tokens\"] += usage.output_tokens if hasattr(usage, 'output_tokens') else 0\n",
    "        else:\n",
    "            message = response.content if hasattr(response, 'content') else str(response)\n",
    "            result = self.anthropic_client.messages.create(\n",
    "                model=self.config.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": message}],\n",
    "                max_tokens=1\n",
    "            )\n",
    "            if hasattr(result, 'usage'):\n",
    "                self.token_usage[\"prompt_tokens\"] += result.usage.input_tokens\n",
    "                self.token_usage[\"completion_tokens\"] += result.usage.output_tokens\n",
    "\n",
    "    def _extract_thought(self, text: str) -> str:\n",
    "        if \"Thought:\" in text:\n",
    "            return text.split(\"Thought:\")[1].split(\"SQL\")[0].strip()\n",
    "        return \"\"\n",
    "\n",
    "    def _extract_sql(self, text: str) -> str:\n",
    "        if \"SQL:\" in text:\n",
    "            sql_part = text.split(\"SQL:\")[1]\n",
    "            if \"SQLResult:\" in sql_part:\n",
    "                return sql_part.split(\"SQLResult:\")[0].strip()\n",
    "            if \"Final Answer:\" in sql_part:\n",
    "                return sql_part.split(\"Final Answer:\")[0].strip()\n",
    "            return sql_part.strip()\n",
    "        return \"\"\n",
    "\n",
    "# Initialize the database schema once at the start\n",
    "config = Config()\n",
    "analyzer = StockAnalyzer(config)\n",
    "schema = analyzer.schema  # Store the schema for later use\n",
    "\n",
    "def format_output(results: Dict) -> str:\n",
    "    output = []\n",
    "    output.append(\"=== Stock Analysis Results ===\")\n",
    "    output.append(f\"\\nQuery: {results.get('user_query', 'N/A')}\")\n",
    "    \n",
    "    output.append(f\"\\nProcessing Time: {results.get('processing_time', 0):.2f} seconds\")\n",
    "    token_usage = results.get('token_usage', {})\n",
    "    output.append(f\"Token Usage:\")\n",
    "    output.append(f\"  Prompt Tokens: {token_usage.get('prompt_tokens', 0)}\")\n",
    "    output.append(f\"  Completion Tokens: {token_usage.get('completion_tokens', 0)}\")\n",
    "    output.append(f\"  Total Tokens: {token_usage.get('prompt_tokens', 0) + token_usage.get('completion_tokens', 0)}\")\n",
    "    \n",
    "    if \"error\" in results:\n",
    "        output.append(f\"\\nError: {results['error']}\")\n",
    "        return \"\\n\".join(output)\n",
    "    \n",
    "    if results.get('query_type') == 'direct_sql':\n",
    "        output.append(f\"\\nThought Process: {results.get('thought_process', 'N/A')}\")\n",
    "        output.append(f\"\\nSQL Query: {results.get('sql_query', 'N/A')}\")\n",
    "        output.append(\"\\nResults:\")\n",
    "        if isinstance(results.get('results'), list):\n",
    "            df = pd.DataFrame(results['results'])\n",
    "            output.append(str(df))\n",
    "        else:\n",
    "            output.append(str(results.get('results', 'No results available')))\n",
    "    else:\n",
    "        output.append(\"\\nSub-Questions:\")\n",
    "        for i, q in enumerate(results.get('sub_questions', []), 1):\n",
    "            output.append(f\"{i}. {q}\")\n",
    "        \n",
    "        output.append(\"\\nSQL Analysis:\")\n",
    "        for key, data in results.get('sql_analysis', {}).items():\n",
    "            output.append(f\"\\nQuestion: {data.get('question', 'N/A')}\")\n",
    "            if 'error' not in data:\n",
    "                output.append(f\"Thought Process: {data.get('thought', 'N/A')}\")\n",
    "                output.append(f\"SQL Query: {data.get('sql', 'N/A')}\")\n",
    "                try:\n",
    "                    if isinstance(data.get('result'), (list, dict)):\n",
    "                        df = pd.DataFrame(data['result'])\n",
    "                        output.append(str(df))\n",
    "                    else:\n",
    "                        output.append(f\"Results: {data.get('result', 'No results available')}\")\n",
    "                except:\n",
    "                    output.append(f\"Results: {data.get('result', 'No results available')}\")\n",
    "            else:\n",
    "                output.append(f\"Error: {data['error']}\")\n",
    "        \n",
    "        output.append(\"\\nExpert Analysis:\")\n",
    "        output.append(results.get('expert_analysis', 'No analysis available'))\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "def analyze_stock_query(query: str) -> str:\n",
    "    try:\n",
    "        config = Config()  # Removed the human_in_the_loop argument\n",
    "        analyzer = StockAnalyzer(config)\n",
    "        results = analyzer.analyze(query)\n",
    "        \n",
    "        if results and \"error\" not in results:\n",
    "            formatted_output = format_output(results)\n",
    "            filename = f\"{query[:50].replace(' ', '_').lower()}_analysis.json\"\n",
    "            return formatted_output + f\"\\n\\nDetailed results saved to {filename}\"\n",
    "        else:\n",
    "            return f\"Error: {results.get('error', 'Unknown error occurred')}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error during analysis: {str(e)}\"\n",
    "\n",
    "# Part 6: Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    test_queries = [\n",
    "        \"look form the final_income_sheet_new_seq and give me the total room revenue for residence inn westshore tampa for the month of June 2024\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\nProcessing: {query}\")\n",
    "        print(\"=\" * 50) \n",
    "        result = analyze_stock_query(query)  # Removed the human_in_the_loop argument\n",
    "        print(result)\n",
    "        print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  # Import the json module for handling JSON data\n",
    "\n",
    "def generate_filename(query: str) -> str:\n",
    "    \"\"\"Generate a filename from the query for the analysis results.\"\"\"\n",
    "    return f\"{query[:50].replace(' ', '_').lower()}_analysis.json\"  # Create a filename based on the query\n",
    "\n",
    "# %% cell 1 code\n",
    "def read_json_file(filename: str) -> dict:\n",
    "    \"\"\"Read and parse the JSON file, returning the data.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)  # Load and return the JSON data\n",
    "    \n",
    "# %% cell 2 code\n",
    "def format_value(value, indent=0):\n",
    "    \"\"\"Recursively format JSON values with proper indentation.\"\"\"\n",
    "    indent_str = \"    \" * indent  # Create indentation string based on the level of nesting\n",
    "    \n",
    "    if isinstance(value, dict):\n",
    "        for k, v in value.items():\n",
    "            key_str = k.replace('_', ' ').title()  # Format the key for display\n",
    "            print(f\"{indent_str}{key_str}:\")\n",
    "            format_value(v, indent + 1)  # Recursively format the value\n",
    "\n",
    "    elif isinstance(value, list):\n",
    "        for item in value:\n",
    "            print(f\"{indent_str}•\", end=' ')  # Bullet point for list items\n",
    "            format_value(item, indent + 1)  # Recursively format each item\n",
    "\n",
    "    elif isinstance(value, (int, float)):\n",
    "        print(f\"{value:,}\")  # Print numbers with commas\n",
    "\n",
    "    elif isinstance(value, bool):\n",
    "        print(str(value))  # Print boolean values\n",
    "\n",
    "    elif value is None:\n",
    "        print(\"None\")  # Print 'None' for NoneType\n",
    "\n",
    "    else:\n",
    "        print(str(value).strip())  # Print string values\n",
    "\n",
    "# %% cell 3 code\n",
    "def display_json_details(query: str) -> None:\n",
    "    \"\"\"Display detailed JSON analysis results in a readable format.\"\"\"\n",
    "    try:\n",
    "        filename = generate_filename(query)  # Generate the filename from the query\n",
    "        data = read_json_file(filename)  # Read and parse the JSON file\n",
    "\n",
    "        print(\"\\n=== DETAILED ANALYSIS REPORT ===\\n\")\n",
    "\n",
    "        # Process each top-level key in the JSON data\n",
    "        for key, value in data.items():\n",
    "            print(f\"\\n{key.replace('_', ' ').title()}:\", end='')  # Format the key for display\n",
    "            format_value(value)  # Format the associated value\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")  # Print a separator\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nError: Analysis file '{filename}' not found\\n\")  # Handle file not found error\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"\\nError: Unable to parse JSON from '{filename}'\\n\")  # Handle JSON parsing error\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError displaying JSON details: {str(e)}\\n\")  # Handle any other exceptions\n",
    "\n",
    "# %% cell 4 code\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    for query in test_queries:\n",
    "        print(f\"\\nDisplaying detailed analysis for: {query}\")  # Indicate which query is being processed\n",
    "        display_json_details(query)  # Call the function to display JSON details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
